---
title: "Modelling Flow"
author: "Matthew Ross"
date: "2024-04-24"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(GGally)
knitr::opts_chunk$set(echo = TRUE)
```

# Modelling Flow

Now that we have explored individual correlations between long-term flow 
characteristics and potential drivers of those characteristics (climate,
soils, etc...), we can start to build out increasingly complex models
to predict flow characteristics. 

# Assignment


## Build a parsimonious linear model

Pick one of the flow characteristics that mosts interests you and use the `lm`
function to build a parsimonious model that predicts your favorite characteristic. What is parsimony? It's a model that is complex enough to
capture the major controls, but simple enough to be understandable. You
will need to explore, read, understand the differences between a + sign interaction, a ":" interaction and a * interaction in `lm` terminology. 

Please report your R2, slope estimates, and p-value of your model and 
write out your interpretation of these numbers. 

```{r}
# Prep Data 
dat_files <- list.files('data',
                        full.names = T)



climate <- read_delim(dat_files[1], delim = ';')

hydro <- read_delim('data/hydro.txt', delim = ';')

cq <- inner_join(climate, hydro %>%
                   select(gauge_id, q95))

# Select only continuous climate variables 
cq2 <- cq %>% select(-c(gauge_id, high_prec_timing, low_prec_timing))
## Selected two variables that were correlated to q95. 

# Look at Correlation matrix
ggpairs(cq2)

# Create additive linear model 
mod1 <- lm(q95 ~ p_mean + low_prec_freq, data = cq2)
summary(mod1)

# Create interaction linear model 
mod2 <- lm(q95 ~ p_mean * low_prec_freq, data = cq2)
summary(mod2)

```

## Interpretation of Results   
The coefficient of 6.566 for precipitation mean suggests that for every one-unit increase in precipitation mean, the value of 195 increases by 6.566 units, when low precipitation frequency is held constant at 0. This effect is statistically significant (p < 2e-16).

The coefficient of 0.026 for low precipitation frequency indicates that for every one-unit increase in low precipitation frequency, the value of 195 increases by 0.026 units, when precipitation is held constant at 0. This effect is also statistically significant (p = 8.77e-07).

The negative coefficient of -0.018 for the interaction term p_mean:low_prec_freq suggests that the effect of precipitation mean on 195 decreases as low_prec_freq increases. This interaction effect is statistically significant (p < 2e-16).

The R-squared value of 0.7779 indicates that approximately 77.79% of the variation in 195 is explained by the predictors and their interaction.

```{r}
q_mean <- read_delim('data/hydro.txt', delim = ';') %>%
  select(gauge_id, q_mean) %>%
  inner_join(read_delim('data/climate.txt', delim = ';')) %>%
  inner_join(read_delim('data/soil.txt',delim = ';'))


q_mean_logs <- q_mean %>%
  mutate(p_log10 = log10(p_mean),
         aridlog10 = log10(aridity),
         q_mean10 = log10(q_mean))

naive_mod <- lm(q_mean10 ~ aridlog10 * p_log10, data = q_mean_logs)
```

 
## Build a CART model to predict flow. 

Linear models help us both predict and understand drivers of change, machine learning can help us understand drivers of change, but as a technique it is 
more suited to accurate predictions. CART or Classification and Regression Trees
are a nice intermediate between lms and ml. Tons of resources for this but
[CART Logic](https://koalaverse.github.io/machine-learning-in-R/decision-trees.html#cart-software-in-r), provides a good conceptual overview, and [CART demo](https://www.statmethods.net/advstats/cart.html) provides a good enough code demo. 

Read the logic intro above, and the code demo as well, to build a CART model 
version of your lm. Use the code to visualize your CART output. 

```{r}
library(rpart)
library(rpart.plot)
library(yardstick)

set.seed(22157)
q_4 <- q_mean_logs %>%
  select(q_mean10, p_log10, aridlog10, soil_depth_pelletier, max_water_content, organic_frac, frac_snow,pet_mean) %>%
  mutate(q_class = cut_number(q_mean10,n = 4)) %>%
  select(-q_mean10) %>%
  na.omit()

train <- q_4 %>%
  sample_frac(.7)

test <- q_4 %>%
  anti_join(train)

cart_simple <- rpart(q_class ~., data = train, cp = 0.01,
                     method = 'class')


plot(cart_simple)
text(cart_simple, cex = 0.8, use.n = TRUE, xpd = TRUE)


test$pred <- predict(cart_simple, test, 'class')
cm <- conf_mat(test, q_class,pred)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")

accuracy(test,q_class,pred)
```


## Build a RandomForest

CARTs are a single tree, what if we had thousands? Would we get better performance (yes!)

The same CART logic site above introduces random forests as well. Please 
read this part of the site and use the code demo to build your own RandomForest.
Remember, for a RandomForest type model we want to make sure we split our data
at least into train and test datasets and ideally into train-test-val. 


```{r}
library(randomForest)
summary(train)
rf_class <- randomForest(q_class ~ ., data = train, 
                         maxnodes = 9, 
                         nPerm = 2,
                         mtry = 5,
                         importance = T)

test$rf_pred <- predict(rf_class, test)

cm_rf <- conf_mat(test, q_class,rf_pred)

autoplot(cm_rf, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")

accuracy(test,q_class,rf_pred)

q_rf <- q_mean_logs %>%
  select(q_mean10, p_log10, aridlog10, soil_depth_pelletier,  frac_snow,pet_mean) %>%
  na.omit()

train_cont <- q_rf %>%
  sample_frac(.7)

test_cont <- q_rf %>%
  anti_join(train)


rf_numer <- randomForest(q_mean10 ~ ., data = train_cont, 
                         maxnodes = 50, 
                         nPerm = 5,
                         mtry = 8,
                         importance = T)

rf_numer$importance

test_cont$rf_pred <- predict(rf_numer, test_cont)
train_cont$rf_pred <- predict(rf_numer, train_cont)


ggplot(test_cont, aes(x = q_mean10,
                      y = rf_pred)) + 
  geom_point() + 
  geom_abline(slope = 1)


cor(test_cont$rf_pred,test_cont$q_mean10)^2
cor(train_cont$rf_pred,train_cont$q_mean10)^2

```

